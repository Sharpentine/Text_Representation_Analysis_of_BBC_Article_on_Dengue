{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Article selection:\n",
        "Title: Philippine town offers bounty for mosquitoes as dengue rises\n",
        "\n",
        "Authors: Virma Simonette & Joel Guinto\n",
        "\n",
        "Publication Date: 19/02/2025\n",
        "\n",
        "Source: BBC News\n",
        "\n",
        "[BBC Article Link](https://www.bbc.com/news/articles/cvgdr4r2l31o)\n"
      ],
      "metadata": {
        "id": "NXVi0Q281Wj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing and Downloading necessary libraries"
      ],
      "metadata": {
        "id": "ZaOIDPrREA--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XuT3uxO1SZg",
        "outputId": "2e1f8ada-f6ab-45b6-f346-a15746f7e5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd5Glwxf6fvy",
        "outputId": "ec831e64-023e-48f4-dcf3-d0317965779f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "71B4B9o76hjM",
        "outputId": "595adfd4-a474-44fc-ff98-11ea266c44ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting beautifulsoup4>=4.4.1 (from newspaper3k)\n",
            "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting Pillow>=3.3.0 (from newspaper3k)\n",
            "  Downloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting PyYAML>=3.11 (from newspaper3k)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting lxml>=3.6.0 (from newspaper3k)\n",
            "  Downloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting nltk>=3.2.1 (from newspaper3k)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting requests>=2.10.0 (from newspaper3k)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dateutil>=2.5.3 (from newspaper3k)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typing-extensions>=4.0.0 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting six (from feedfinder2>=0.0.4->newspaper3k)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk>=3.2.1->newspaper3k)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.10.0->newspaper3k)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.0/186.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=25481bfdcff7b02cfbb95c7276df7f18e76c79bef68ac3b0dece372f3580fc71\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=5707b0b6dd12559a27295c8a50a4a2fe687a722d5e358e2f38cc68026908ed1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=b7148af3a9bd25eefa682f3053b024da67a634cf2990098664be47e9df375940\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=b6f1de668cc9c6c9dfb02f0ffd75c84545a7518d14ffed22dee08c26a867af70\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, urllib3, typing-extensions, tqdm, soupsieve, six, regex, PyYAML, Pillow, lxml, joblib, idna, filelock, feedparser, cssselect, click, charset-normalizer, certifi, requests, python-dateutil, nltk, beautifulsoup4, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.6\n",
            "    Uninstalling soupsieve-2.6:\n",
            "      Successfully uninstalled soupsieve-2.6\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.3.1\n",
            "    Uninstalling lxml-5.3.1:\n",
            "      Successfully uninstalled lxml-5.3.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.17.0\n",
            "    Uninstalling filelock-3.17.0:\n",
            "      Successfully uninstalled filelock-3.17.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.3\n",
            "    Uninstalling beautifulsoup4-4.13.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-11.1.0 PyYAML-6.0.2 beautifulsoup4-4.13.3 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.17.0 idna-3.10 jieba3k-0.35.1 joblib-1.4.2 lxml-5.3.1 newspaper3k-0.2.8 nltk-3.9.1 python-dateutil-2.9.0.post0 regex-2024.11.6 requests-2.32.3 requests-file-2.1.0 sgmllib3k-1.0.0 six-1.17.0 soupsieve-2.6 tinysegmenter-0.3 tldextract-5.1.3 tqdm-4.67.1 typing-extensions-4.12.2 urllib3-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "joblib",
                  "lxml",
                  "nltk",
                  "regex",
                  "requests",
                  "six"
                ]
              },
              "id": "7e9748556a24431eacaadc86127775e8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article"
      ],
      "metadata": {
        "id": "vcrAcb3y6oFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the Article from the URL"
      ],
      "metadata": {
        "id": "s5XzXYgXB_Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://www.bbc.com/news/articles/cvgdr4r2l31o'\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "print(\"Title:\", article.title)\n",
        "article1=article.text\n",
        "print(article1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXauc8ER4wUM",
        "outputId": "ab02de49-e3de-4c9a-f5a0-41183b872e45"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Philippines: Manila town offers bounty for mosquitoes as dengue rises\n",
            "Philippine town offers bounty for mosquitoes as dengue rises\n",
            "\n",
            "The move follows a recent spike in cases of dengue, which is spread by mosquitoes, in the Philippines.\n",
            "\n",
            "While news of the bounty has provoked scorn on social media, Mr Cernal has defended it as necessary for the community's health.\n",
            "\n",
            "Carlito Cernal, village chief of Barangay Addition Hills in central Manila, announced the bounty of one peso (less than two US cents) for every five mosquitoes.\n",
            "\n",
            "Authorities in one of the Philippines' most densely-populated urban centres are offering a cash reward for mosquitoes in an attempt to stop the spread of dengue.\n",
            "\n",
            "The programme, which will run for at least a month, was started after two students in Mr Cernal's neighbourhood died from the disease.\n",
            "\n",
            "The bounty applies to all mosquitoes - dead or alive - and their larvae, Mr Cernal added. Live mosquitoes will be exterminated using ultraviolet light.\n",
            "\n",
            "A total of 21 people have already claimed their reward, bringing in a total of 700 mosquitoes and larvae so far, he told the BBC.\n",
            "\n",
            "The bounty drew swift ridicule after it was announced late on Tuesday.\n",
            "\n",
            "\"Mosquito farming is coming,\" one social media comment read. \"Will a mosquito get rejected if it has only one wing?\" read another.\n",
            "\n",
            "The Philippines' Department of Health (DOH) told the BBC that it \"appreciates the good intentions of local government executives to fight dengue\".\n",
            "\n",
            "It declined further comment, however, when asked if catching mosquitoes in exchange for cash is an effective way of stopping dengue.\n",
            "\n",
            "\"We urge all concerned to please consult and coordinate with their local health officers or the DOH regional office in their area for evidence-based practices that are known to work,\" it said.\n",
            "\n",
            "Mr Cernal said he was aware that the bounty had been bashed on social media, but added: \"This is one of the biggest and most dense areas. We have to do something to help the local government.\"\n",
            "\n",
            "He pointed out that local health authorities recorded 44 cases of dengue in the community during the most recent surge of infections.\n",
            "\n",
            "Barangay Addition Hills is home to nearly 70,000 people, crammed into a 162-hectare patch at the heart of the capital, Metro Manila.\n",
            "\n",
            "Mr Cernal said the bounty was meant to supplement existing measures such as cleaning the streets and preventing the build-up of water where dengue-carrying mosquitoes lay their eggs.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0cf64Sy72Pt",
        "outputId": "184cc85d-0408-416d-846c-99386c2038d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "ZpBybBvHDP_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "Dividing the article into sentences and words."
      ],
      "metadata": {
        "id": "7RKeoND6B0x_"
      }
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Tokenize the article into sentences\n",
        "sentences = sent_tokenize(article1)\n",
        "\n",
        "# Tokenize the article into words\n",
        "words = word_tokenize(article1)\n",
        "\n",
        "# Print the sentences\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# Print the words\n",
        "print(\"\\nWords:\")\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnykbH8I72dU",
        "outputId": "6a2694eb-21c2-426e-e97d-ebdbc9d42c51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "Philippine town offers bounty for mosquitoes as dengue rises\n",
            "\n",
            "The move follows a recent spike in cases of dengue, which is spread by mosquitoes, in the Philippines.\n",
            "While news of the bounty has provoked scorn on social media, Mr Cernal has defended it as necessary for the community's health.\n",
            "Carlito Cernal, village chief of Barangay Addition Hills in central Manila, announced the bounty of one peso (less than two US cents) for every five mosquitoes.\n",
            "Authorities in one of the Philippines' most densely-populated urban centres are offering a cash reward for mosquitoes in an attempt to stop the spread of dengue.\n",
            "The programme, which will run for at least a month, was started after two students in Mr Cernal's neighbourhood died from the disease.\n",
            "The bounty applies to all mosquitoes - dead or alive - and their larvae, Mr Cernal added.\n",
            "Live mosquitoes will be exterminated using ultraviolet light.\n",
            "A total of 21 people have already claimed their reward, bringing in a total of 700 mosquitoes and larvae so far, he told the BBC.\n",
            "The bounty drew swift ridicule after it was announced late on Tuesday.\n",
            "\"Mosquito farming is coming,\" one social media comment read.\n",
            "\"Will a mosquito get rejected if it has only one wing?\"\n",
            "read another.\n",
            "The Philippines' Department of Health (DOH) told the BBC that it \"appreciates the good intentions of local government executives to fight dengue\".\n",
            "It declined further comment, however, when asked if catching mosquitoes in exchange for cash is an effective way of stopping dengue.\n",
            "\"We urge all concerned to please consult and coordinate with their local health officers or the DOH regional office in their area for evidence-based practices that are known to work,\" it said.\n",
            "Mr Cernal said he was aware that the bounty had been bashed on social media, but added: \"This is one of the biggest and most dense areas.\n",
            "We have to do something to help the local government.\"\n",
            "He pointed out that local health authorities recorded 44 cases of dengue in the community during the most recent surge of infections.\n",
            "Barangay Addition Hills is home to nearly 70,000 people, crammed into a 162-hectare patch at the heart of the capital, Metro Manila.\n",
            "Mr Cernal said the bounty was meant to supplement existing measures such as cleaning the streets and preventing the build-up of water where dengue-carrying mosquitoes lay their eggs.\n",
            "\n",
            "Words:\n",
            "Philippine\n",
            "town\n",
            "offers\n",
            "bounty\n",
            "for\n",
            "mosquitoes\n",
            "as\n",
            "dengue\n",
            "rises\n",
            "The\n",
            "move\n",
            "follows\n",
            "a\n",
            "recent\n",
            "spike\n",
            "in\n",
            "cases\n",
            "of\n",
            "dengue\n",
            ",\n",
            "which\n",
            "is\n",
            "spread\n",
            "by\n",
            "mosquitoes\n",
            ",\n",
            "in\n",
            "the\n",
            "Philippines\n",
            ".\n",
            "While\n",
            "news\n",
            "of\n",
            "the\n",
            "bounty\n",
            "has\n",
            "provoked\n",
            "scorn\n",
            "on\n",
            "social\n",
            "media\n",
            ",\n",
            "Mr\n",
            "Cernal\n",
            "has\n",
            "defended\n",
            "it\n",
            "as\n",
            "necessary\n",
            "for\n",
            "the\n",
            "community\n",
            "'s\n",
            "health\n",
            ".\n",
            "Carlito\n",
            "Cernal\n",
            ",\n",
            "village\n",
            "chief\n",
            "of\n",
            "Barangay\n",
            "Addition\n",
            "Hills\n",
            "in\n",
            "central\n",
            "Manila\n",
            ",\n",
            "announced\n",
            "the\n",
            "bounty\n",
            "of\n",
            "one\n",
            "peso\n",
            "(\n",
            "less\n",
            "than\n",
            "two\n",
            "US\n",
            "cents\n",
            ")\n",
            "for\n",
            "every\n",
            "five\n",
            "mosquitoes\n",
            ".\n",
            "Authorities\n",
            "in\n",
            "one\n",
            "of\n",
            "the\n",
            "Philippines\n",
            "'\n",
            "most\n",
            "densely-populated\n",
            "urban\n",
            "centres\n",
            "are\n",
            "offering\n",
            "a\n",
            "cash\n",
            "reward\n",
            "for\n",
            "mosquitoes\n",
            "in\n",
            "an\n",
            "attempt\n",
            "to\n",
            "stop\n",
            "the\n",
            "spread\n",
            "of\n",
            "dengue\n",
            ".\n",
            "The\n",
            "programme\n",
            ",\n",
            "which\n",
            "will\n",
            "run\n",
            "for\n",
            "at\n",
            "least\n",
            "a\n",
            "month\n",
            ",\n",
            "was\n",
            "started\n",
            "after\n",
            "two\n",
            "students\n",
            "in\n",
            "Mr\n",
            "Cernal\n",
            "'s\n",
            "neighbourhood\n",
            "died\n",
            "from\n",
            "the\n",
            "disease\n",
            ".\n",
            "The\n",
            "bounty\n",
            "applies\n",
            "to\n",
            "all\n",
            "mosquitoes\n",
            "-\n",
            "dead\n",
            "or\n",
            "alive\n",
            "-\n",
            "and\n",
            "their\n",
            "larvae\n",
            ",\n",
            "Mr\n",
            "Cernal\n",
            "added\n",
            ".\n",
            "Live\n",
            "mosquitoes\n",
            "will\n",
            "be\n",
            "exterminated\n",
            "using\n",
            "ultraviolet\n",
            "light\n",
            ".\n",
            "A\n",
            "total\n",
            "of\n",
            "21\n",
            "people\n",
            "have\n",
            "already\n",
            "claimed\n",
            "their\n",
            "reward\n",
            ",\n",
            "bringing\n",
            "in\n",
            "a\n",
            "total\n",
            "of\n",
            "700\n",
            "mosquitoes\n",
            "and\n",
            "larvae\n",
            "so\n",
            "far\n",
            ",\n",
            "he\n",
            "told\n",
            "the\n",
            "BBC\n",
            ".\n",
            "The\n",
            "bounty\n",
            "drew\n",
            "swift\n",
            "ridicule\n",
            "after\n",
            "it\n",
            "was\n",
            "announced\n",
            "late\n",
            "on\n",
            "Tuesday\n",
            ".\n",
            "``\n",
            "Mosquito\n",
            "farming\n",
            "is\n",
            "coming\n",
            ",\n",
            "''\n",
            "one\n",
            "social\n",
            "media\n",
            "comment\n",
            "read\n",
            ".\n",
            "``\n",
            "Will\n",
            "a\n",
            "mosquito\n",
            "get\n",
            "rejected\n",
            "if\n",
            "it\n",
            "has\n",
            "only\n",
            "one\n",
            "wing\n",
            "?\n",
            "''\n",
            "read\n",
            "another\n",
            ".\n",
            "The\n",
            "Philippines\n",
            "'\n",
            "Department\n",
            "of\n",
            "Health\n",
            "(\n",
            "DOH\n",
            ")\n",
            "told\n",
            "the\n",
            "BBC\n",
            "that\n",
            "it\n",
            "``\n",
            "appreciates\n",
            "the\n",
            "good\n",
            "intentions\n",
            "of\n",
            "local\n",
            "government\n",
            "executives\n",
            "to\n",
            "fight\n",
            "dengue\n",
            "''\n",
            ".\n",
            "It\n",
            "declined\n",
            "further\n",
            "comment\n",
            ",\n",
            "however\n",
            ",\n",
            "when\n",
            "asked\n",
            "if\n",
            "catching\n",
            "mosquitoes\n",
            "in\n",
            "exchange\n",
            "for\n",
            "cash\n",
            "is\n",
            "an\n",
            "effective\n",
            "way\n",
            "of\n",
            "stopping\n",
            "dengue\n",
            ".\n",
            "``\n",
            "We\n",
            "urge\n",
            "all\n",
            "concerned\n",
            "to\n",
            "please\n",
            "consult\n",
            "and\n",
            "coordinate\n",
            "with\n",
            "their\n",
            "local\n",
            "health\n",
            "officers\n",
            "or\n",
            "the\n",
            "DOH\n",
            "regional\n",
            "office\n",
            "in\n",
            "their\n",
            "area\n",
            "for\n",
            "evidence-based\n",
            "practices\n",
            "that\n",
            "are\n",
            "known\n",
            "to\n",
            "work\n",
            ",\n",
            "''\n",
            "it\n",
            "said\n",
            ".\n",
            "Mr\n",
            "Cernal\n",
            "said\n",
            "he\n",
            "was\n",
            "aware\n",
            "that\n",
            "the\n",
            "bounty\n",
            "had\n",
            "been\n",
            "bashed\n",
            "on\n",
            "social\n",
            "media\n",
            ",\n",
            "but\n",
            "added\n",
            ":\n",
            "``\n",
            "This\n",
            "is\n",
            "one\n",
            "of\n",
            "the\n",
            "biggest\n",
            "and\n",
            "most\n",
            "dense\n",
            "areas\n",
            ".\n",
            "We\n",
            "have\n",
            "to\n",
            "do\n",
            "something\n",
            "to\n",
            "help\n",
            "the\n",
            "local\n",
            "government\n",
            ".\n",
            "''\n",
            "He\n",
            "pointed\n",
            "out\n",
            "that\n",
            "local\n",
            "health\n",
            "authorities\n",
            "recorded\n",
            "44\n",
            "cases\n",
            "of\n",
            "dengue\n",
            "in\n",
            "the\n",
            "community\n",
            "during\n",
            "the\n",
            "most\n",
            "recent\n",
            "surge\n",
            "of\n",
            "infections\n",
            ".\n",
            "Barangay\n",
            "Addition\n",
            "Hills\n",
            "is\n",
            "home\n",
            "to\n",
            "nearly\n",
            "70,000\n",
            "people\n",
            ",\n",
            "crammed\n",
            "into\n",
            "a\n",
            "162-hectare\n",
            "patch\n",
            "at\n",
            "the\n",
            "heart\n",
            "of\n",
            "the\n",
            "capital\n",
            ",\n",
            "Metro\n",
            "Manila\n",
            ".\n",
            "Mr\n",
            "Cernal\n",
            "said\n",
            "the\n",
            "bounty\n",
            "was\n",
            "meant\n",
            "to\n",
            "supplement\n",
            "existing\n",
            "measures\n",
            "such\n",
            "as\n",
            "cleaning\n",
            "the\n",
            "streets\n",
            "and\n",
            "preventing\n",
            "the\n",
            "build-up\n",
            "of\n",
            "water\n",
            "where\n",
            "dengue-carrying\n",
            "mosquitoes\n",
            "lay\n",
            "their\n",
            "eggs\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopword Removal\n",
        "Eliminating common words that may not carry significant meaning (e.g., \"the\", \"is\", \"and\")."
      ],
      "metadata": {
        "id": "WjYKO3HECFcA"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Tokenize the article into sentences\n",
        "sentences = sent_tokenize(article1)\n",
        "\n",
        "# Tokenize the article into words and remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in word_tokenize(article1) if word.lower() not in stop_words]\n",
        "\n",
        "# Print the sentences\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# Print the words (excluding stop words)\n",
        "print(\"\\nWords:\")\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-rifieY8BTt",
        "outputId": "752adf50-89be-4739-8684-27fde301c607"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "Philippine town offers bounty for mosquitoes as dengue rises\n",
            "\n",
            "The move follows a recent spike in cases of dengue, which is spread by mosquitoes, in the Philippines.\n",
            "While news of the bounty has provoked scorn on social media, Mr Cernal has defended it as necessary for the community's health.\n",
            "Carlito Cernal, village chief of Barangay Addition Hills in central Manila, announced the bounty of one peso (less than two US cents) for every five mosquitoes.\n",
            "Authorities in one of the Philippines' most densely-populated urban centres are offering a cash reward for mosquitoes in an attempt to stop the spread of dengue.\n",
            "The programme, which will run for at least a month, was started after two students in Mr Cernal's neighbourhood died from the disease.\n",
            "The bounty applies to all mosquitoes - dead or alive - and their larvae, Mr Cernal added.\n",
            "Live mosquitoes will be exterminated using ultraviolet light.\n",
            "A total of 21 people have already claimed their reward, bringing in a total of 700 mosquitoes and larvae so far, he told the BBC.\n",
            "The bounty drew swift ridicule after it was announced late on Tuesday.\n",
            "\"Mosquito farming is coming,\" one social media comment read.\n",
            "\"Will a mosquito get rejected if it has only one wing?\"\n",
            "read another.\n",
            "The Philippines' Department of Health (DOH) told the BBC that it \"appreciates the good intentions of local government executives to fight dengue\".\n",
            "It declined further comment, however, when asked if catching mosquitoes in exchange for cash is an effective way of stopping dengue.\n",
            "\"We urge all concerned to please consult and coordinate with their local health officers or the DOH regional office in their area for evidence-based practices that are known to work,\" it said.\n",
            "Mr Cernal said he was aware that the bounty had been bashed on social media, but added: \"This is one of the biggest and most dense areas.\n",
            "We have to do something to help the local government.\"\n",
            "He pointed out that local health authorities recorded 44 cases of dengue in the community during the most recent surge of infections.\n",
            "Barangay Addition Hills is home to nearly 70,000 people, crammed into a 162-hectare patch at the heart of the capital, Metro Manila.\n",
            "Mr Cernal said the bounty was meant to supplement existing measures such as cleaning the streets and preventing the build-up of water where dengue-carrying mosquitoes lay their eggs.\n",
            "\n",
            "Words:\n",
            "Philippine\n",
            "town\n",
            "offers\n",
            "bounty\n",
            "mosquitoes\n",
            "dengue\n",
            "rises\n",
            "move\n",
            "follows\n",
            "recent\n",
            "spike\n",
            "cases\n",
            "dengue\n",
            ",\n",
            "spread\n",
            "mosquitoes\n",
            ",\n",
            "Philippines\n",
            ".\n",
            "news\n",
            "bounty\n",
            "provoked\n",
            "scorn\n",
            "social\n",
            "media\n",
            ",\n",
            "Mr\n",
            "Cernal\n",
            "defended\n",
            "necessary\n",
            "community\n",
            "'s\n",
            "health\n",
            ".\n",
            "Carlito\n",
            "Cernal\n",
            ",\n",
            "village\n",
            "chief\n",
            "Barangay\n",
            "Addition\n",
            "Hills\n",
            "central\n",
            "Manila\n",
            ",\n",
            "announced\n",
            "bounty\n",
            "one\n",
            "peso\n",
            "(\n",
            "less\n",
            "two\n",
            "US\n",
            "cents\n",
            ")\n",
            "every\n",
            "five\n",
            "mosquitoes\n",
            ".\n",
            "Authorities\n",
            "one\n",
            "Philippines\n",
            "'\n",
            "densely-populated\n",
            "urban\n",
            "centres\n",
            "offering\n",
            "cash\n",
            "reward\n",
            "mosquitoes\n",
            "attempt\n",
            "stop\n",
            "spread\n",
            "dengue\n",
            ".\n",
            "programme\n",
            ",\n",
            "run\n",
            "least\n",
            "month\n",
            ",\n",
            "started\n",
            "two\n",
            "students\n",
            "Mr\n",
            "Cernal\n",
            "'s\n",
            "neighbourhood\n",
            "died\n",
            "disease\n",
            ".\n",
            "bounty\n",
            "applies\n",
            "mosquitoes\n",
            "-\n",
            "dead\n",
            "alive\n",
            "-\n",
            "larvae\n",
            ",\n",
            "Mr\n",
            "Cernal\n",
            "added\n",
            ".\n",
            "Live\n",
            "mosquitoes\n",
            "exterminated\n",
            "using\n",
            "ultraviolet\n",
            "light\n",
            ".\n",
            "total\n",
            "21\n",
            "people\n",
            "already\n",
            "claimed\n",
            "reward\n",
            ",\n",
            "bringing\n",
            "total\n",
            "700\n",
            "mosquitoes\n",
            "larvae\n",
            "far\n",
            ",\n",
            "told\n",
            "BBC\n",
            ".\n",
            "bounty\n",
            "drew\n",
            "swift\n",
            "ridicule\n",
            "announced\n",
            "late\n",
            "Tuesday\n",
            ".\n",
            "``\n",
            "Mosquito\n",
            "farming\n",
            "coming\n",
            ",\n",
            "''\n",
            "one\n",
            "social\n",
            "media\n",
            "comment\n",
            "read\n",
            ".\n",
            "``\n",
            "mosquito\n",
            "get\n",
            "rejected\n",
            "one\n",
            "wing\n",
            "?\n",
            "''\n",
            "read\n",
            "another\n",
            ".\n",
            "Philippines\n",
            "'\n",
            "Department\n",
            "Health\n",
            "(\n",
            "DOH\n",
            ")\n",
            "told\n",
            "BBC\n",
            "``\n",
            "appreciates\n",
            "good\n",
            "intentions\n",
            "local\n",
            "government\n",
            "executives\n",
            "fight\n",
            "dengue\n",
            "''\n",
            ".\n",
            "declined\n",
            "comment\n",
            ",\n",
            "however\n",
            ",\n",
            "asked\n",
            "catching\n",
            "mosquitoes\n",
            "exchange\n",
            "cash\n",
            "effective\n",
            "way\n",
            "stopping\n",
            "dengue\n",
            ".\n",
            "``\n",
            "urge\n",
            "concerned\n",
            "please\n",
            "consult\n",
            "coordinate\n",
            "local\n",
            "health\n",
            "officers\n",
            "DOH\n",
            "regional\n",
            "office\n",
            "area\n",
            "evidence-based\n",
            "practices\n",
            "known\n",
            "work\n",
            ",\n",
            "''\n",
            "said\n",
            ".\n",
            "Mr\n",
            "Cernal\n",
            "said\n",
            "aware\n",
            "bounty\n",
            "bashed\n",
            "social\n",
            "media\n",
            ",\n",
            "added\n",
            ":\n",
            "``\n",
            "one\n",
            "biggest\n",
            "dense\n",
            "areas\n",
            ".\n",
            "something\n",
            "help\n",
            "local\n",
            "government\n",
            ".\n",
            "''\n",
            "pointed\n",
            "local\n",
            "health\n",
            "authorities\n",
            "recorded\n",
            "44\n",
            "cases\n",
            "dengue\n",
            "community\n",
            "recent\n",
            "surge\n",
            "infections\n",
            ".\n",
            "Barangay\n",
            "Addition\n",
            "Hills\n",
            "home\n",
            "nearly\n",
            "70,000\n",
            "people\n",
            ",\n",
            "crammed\n",
            "162-hectare\n",
            "patch\n",
            "heart\n",
            "capital\n",
            ",\n",
            "Metro\n",
            "Manila\n",
            ".\n",
            "Mr\n",
            "Cernal\n",
            "said\n",
            "bounty\n",
            "meant\n",
            "supplement\n",
            "existing\n",
            "measures\n",
            "cleaning\n",
            "streets\n",
            "preventing\n",
            "build-up\n",
            "water\n",
            "dengue-carrying\n",
            "mosquitoes\n",
            "lay\n",
            "eggs\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "Converting text to lowercase, remove punctuation, and apply stemming or lemmatization to reduce words to their base forms."
      ],
      "metadata": {
        "id": "2mAVc_iaCTMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "sentences = sent_tokenize(article1)\n",
        "\n",
        "# Preprocessing: lowercase, remove punctuation, and lemmatize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = []\n",
        "for word in word_tokenize(article1):\n",
        "    word = word.lower()  # Convert to lowercase\n",
        "    word = word.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    if word and word not in stop_words:  # Check if word is not empty and not a stop word\n",
        "        word = lemmatizer.lemmatize(word)  # Lemmatize the word\n",
        "        words.append(word)\n",
        "\n",
        "# Print the sentences\n",
        "print(\"Sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "\n",
        "# Print the processed words\n",
        "print(\"\\nProcessed Words:\")\n",
        "for word in words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTWDA5iv8KjW",
        "outputId": "215bafac-c435-4259-9360-6bf9c0b49b2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "Philippine town offers bounty for mosquitoes as dengue rises\n",
            "\n",
            "The move follows a recent spike in cases of dengue, which is spread by mosquitoes, in the Philippines.\n",
            "While news of the bounty has provoked scorn on social media, Mr Cernal has defended it as necessary for the community's health.\n",
            "Carlito Cernal, village chief of Barangay Addition Hills in central Manila, announced the bounty of one peso (less than two US cents) for every five mosquitoes.\n",
            "Authorities in one of the Philippines' most densely-populated urban centres are offering a cash reward for mosquitoes in an attempt to stop the spread of dengue.\n",
            "The programme, which will run for at least a month, was started after two students in Mr Cernal's neighbourhood died from the disease.\n",
            "The bounty applies to all mosquitoes - dead or alive - and their larvae, Mr Cernal added.\n",
            "Live mosquitoes will be exterminated using ultraviolet light.\n",
            "A total of 21 people have already claimed their reward, bringing in a total of 700 mosquitoes and larvae so far, he told the BBC.\n",
            "The bounty drew swift ridicule after it was announced late on Tuesday.\n",
            "\"Mosquito farming is coming,\" one social media comment read.\n",
            "\"Will a mosquito get rejected if it has only one wing?\"\n",
            "read another.\n",
            "The Philippines' Department of Health (DOH) told the BBC that it \"appreciates the good intentions of local government executives to fight dengue\".\n",
            "It declined further comment, however, when asked if catching mosquitoes in exchange for cash is an effective way of stopping dengue.\n",
            "\"We urge all concerned to please consult and coordinate with their local health officers or the DOH regional office in their area for evidence-based practices that are known to work,\" it said.\n",
            "Mr Cernal said he was aware that the bounty had been bashed on social media, but added: \"This is one of the biggest and most dense areas.\n",
            "We have to do something to help the local government.\"\n",
            "He pointed out that local health authorities recorded 44 cases of dengue in the community during the most recent surge of infections.\n",
            "Barangay Addition Hills is home to nearly 70,000 people, crammed into a 162-hectare patch at the heart of the capital, Metro Manila.\n",
            "Mr Cernal said the bounty was meant to supplement existing measures such as cleaning the streets and preventing the build-up of water where dengue-carrying mosquitoes lay their eggs.\n",
            "\n",
            "Processed Words:\n",
            "philippine\n",
            "town\n",
            "offer\n",
            "bounty\n",
            "mosquito\n",
            "dengue\n",
            "rise\n",
            "move\n",
            "follows\n",
            "recent\n",
            "spike\n",
            "case\n",
            "dengue\n",
            "spread\n",
            "mosquito\n",
            "philippine\n",
            "news\n",
            "bounty\n",
            "provoked\n",
            "scorn\n",
            "social\n",
            "medium\n",
            "mr\n",
            "cernal\n",
            "defended\n",
            "necessary\n",
            "community\n",
            "health\n",
            "carlito\n",
            "cernal\n",
            "village\n",
            "chief\n",
            "barangay\n",
            "addition\n",
            "hill\n",
            "central\n",
            "manila\n",
            "announced\n",
            "bounty\n",
            "one\n",
            "peso\n",
            "less\n",
            "two\n",
            "u\n",
            "cent\n",
            "every\n",
            "five\n",
            "mosquito\n",
            "authority\n",
            "one\n",
            "philippine\n",
            "denselypopulated\n",
            "urban\n",
            "centre\n",
            "offering\n",
            "cash\n",
            "reward\n",
            "mosquito\n",
            "attempt\n",
            "stop\n",
            "spread\n",
            "dengue\n",
            "programme\n",
            "run\n",
            "least\n",
            "month\n",
            "started\n",
            "two\n",
            "student\n",
            "mr\n",
            "cernal\n",
            "neighbourhood\n",
            "died\n",
            "disease\n",
            "bounty\n",
            "applies\n",
            "mosquito\n",
            "dead\n",
            "alive\n",
            "larva\n",
            "mr\n",
            "cernal\n",
            "added\n",
            "live\n",
            "mosquito\n",
            "exterminated\n",
            "using\n",
            "ultraviolet\n",
            "light\n",
            "total\n",
            "21\n",
            "people\n",
            "already\n",
            "claimed\n",
            "reward\n",
            "bringing\n",
            "total\n",
            "700\n",
            "mosquito\n",
            "larva\n",
            "far\n",
            "told\n",
            "bbc\n",
            "bounty\n",
            "drew\n",
            "swift\n",
            "ridicule\n",
            "announced\n",
            "late\n",
            "tuesday\n",
            "mosquito\n",
            "farming\n",
            "coming\n",
            "one\n",
            "social\n",
            "medium\n",
            "comment\n",
            "read\n",
            "mosquito\n",
            "get\n",
            "rejected\n",
            "one\n",
            "wing\n",
            "read\n",
            "another\n",
            "philippine\n",
            "department\n",
            "health\n",
            "doh\n",
            "told\n",
            "bbc\n",
            "appreciates\n",
            "good\n",
            "intention\n",
            "local\n",
            "government\n",
            "executive\n",
            "fight\n",
            "dengue\n",
            "declined\n",
            "comment\n",
            "however\n",
            "asked\n",
            "catching\n",
            "mosquito\n",
            "exchange\n",
            "cash\n",
            "effective\n",
            "way\n",
            "stopping\n",
            "dengue\n",
            "urge\n",
            "concerned\n",
            "please\n",
            "consult\n",
            "coordinate\n",
            "local\n",
            "health\n",
            "officer\n",
            "doh\n",
            "regional\n",
            "office\n",
            "area\n",
            "evidencebased\n",
            "practice\n",
            "known\n",
            "work\n",
            "said\n",
            "mr\n",
            "cernal\n",
            "said\n",
            "aware\n",
            "bounty\n",
            "bashed\n",
            "social\n",
            "medium\n",
            "added\n",
            "one\n",
            "biggest\n",
            "dense\n",
            "area\n",
            "something\n",
            "help\n",
            "local\n",
            "government\n",
            "pointed\n",
            "local\n",
            "health\n",
            "authority\n",
            "recorded\n",
            "44\n",
            "case\n",
            "dengue\n",
            "community\n",
            "recent\n",
            "surge\n",
            "infection\n",
            "barangay\n",
            "addition\n",
            "hill\n",
            "home\n",
            "nearly\n",
            "70000\n",
            "people\n",
            "crammed\n",
            "162hectare\n",
            "patch\n",
            "heart\n",
            "capital\n",
            "metro\n",
            "manila\n",
            "mr\n",
            "cernal\n",
            "said\n",
            "bounty\n",
            "meant\n",
            "supplement\n",
            "existing\n",
            "measure\n",
            "cleaning\n",
            "street\n",
            "preventing\n",
            "buildup\n",
            "water\n",
            "denguecarrying\n",
            "mosquito\n",
            "lay\n",
            "egg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction:"
      ],
      "metadata": {
        "id": "kMOfzy8ICjIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (BoW)\n",
        "Creating a BoW representation of the article."
      ],
      "metadata": {
        "id": "Wiv2ydauDjIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lowercase, remove punctuation, lemmatize, and remove stop words.\"\"\"\n",
        "    words = []\n",
        "    for word in word_tokenize(text):\n",
        "        word = word.lower()\n",
        "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "        if word and word not in stop_words:\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            words.append(word)\n",
        "    return ' '.join(words)  # Join words back into a string\n",
        "\n",
        "# Preprocess the article text\n",
        "processed_article = preprocess_text(article1)\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the processed article text\n",
        "vectorizer.fit([processed_article])\n",
        "\n",
        "# Transform the processed article text into a BoW representation\n",
        "bow_representation = vectorizer.transform([processed_article])\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the BoW representation (sparse matrix)\n",
        "print(\"Bag of Words Representation:\")\n",
        "print(bow_representation.toarray())\n",
        "\n",
        "# Print the vocabulary\n",
        "print(\"\\nVocabulary:\")\n",
        "print(feature_names)\n",
        "\n",
        "# Example: Get the count of a specific word\n",
        "word_index = vectorizer.vocabulary_.get('mosquito')\n",
        "if word_index is not None:\n",
        "    word_count = bow_representation[0, word_index]\n",
        "    print(f\"\\nCount of 'mosquito': {word_count}\")\n",
        "else:\n",
        "    print(\"\\n'mosquito' not found in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "may1JRkt8jmA",
        "outputId": "3e8e7e0d-406a-4df8-da03-95a43057e313"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Representation:\n",
            "[[ 1  1  1  1  1  2  2  1  1  2  1  1  1  2  1  1  2  1  2  1  2  1  7  1\n",
            "   1  1  1  2  2  1  1  1  1  6  1  1  1  1  2  2  1  1  1  1  1  1  1  6\n",
            "   1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2\n",
            "   4  1  1  2  1  1  1  1  1  2  1  1  1  1  1  1  4  2  1  1  3  1  1 11\n",
            "   1  5  1  1  1  1  1  1  1  1  5  1  2  1  4  1  1  1  1  1  1  2  2  1\n",
            "   1  1  2  1  1  1  3  1  3  1  1  2  1  1  1  1  1  1  1  1  2  2  1  1\n",
            "   2  1  1  1  1  1  1  1  1  1]]\n",
            "\n",
            "Vocabulary:\n",
            "['162hectare' '21' '44' '700' '70000' 'added' 'addition' 'alive' 'already'\n",
            " 'announced' 'another' 'applies' 'appreciates' 'area' 'asked' 'attempt'\n",
            " 'authority' 'aware' 'barangay' 'bashed' 'bbc' 'biggest' 'bounty'\n",
            " 'bringing' 'buildup' 'capital' 'carlito' 'case' 'cash' 'catching' 'cent'\n",
            " 'central' 'centre' 'cernal' 'chief' 'claimed' 'cleaning' 'coming'\n",
            " 'comment' 'community' 'concerned' 'consult' 'coordinate' 'crammed' 'dead'\n",
            " 'declined' 'defended' 'dengue' 'denguecarrying' 'dense'\n",
            " 'denselypopulated' 'department' 'died' 'disease' 'doh' 'drew' 'effective'\n",
            " 'egg' 'every' 'evidencebased' 'exchange' 'executive' 'existing'\n",
            " 'exterminated' 'far' 'farming' 'fight' 'five' 'follows' 'get' 'good'\n",
            " 'government' 'health' 'heart' 'help' 'hill' 'home' 'however' 'infection'\n",
            " 'intention' 'known' 'larva' 'late' 'lay' 'least' 'less' 'light' 'live'\n",
            " 'local' 'manila' 'meant' 'measure' 'medium' 'metro' 'month' 'mosquito'\n",
            " 'move' 'mr' 'nearly' 'necessary' 'neighbourhood' 'news' 'offer'\n",
            " 'offering' 'office' 'officer' 'one' 'patch' 'people' 'peso' 'philippine'\n",
            " 'please' 'pointed' 'practice' 'preventing' 'programme' 'provoked' 'read'\n",
            " 'recent' 'recorded' 'regional' 'rejected' 'reward' 'ridicule' 'rise'\n",
            " 'run' 'said' 'scorn' 'social' 'something' 'spike' 'spread' 'started'\n",
            " 'stop' 'stopping' 'street' 'student' 'supplement' 'surge' 'swift' 'told'\n",
            " 'total' 'town' 'tuesday' 'two' 'ultraviolet' 'urban' 'urge' 'using'\n",
            " 'village' 'water' 'way' 'wing' 'work']\n",
            "\n",
            "Count of 'mosquito': 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "Computing Term Frequency-Inverse Document Frequency scores for the article."
      ],
      "metadata": {
        "id": "T1NzWzWFDuqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lowercase, remove punctuation, lemmatize, and remove stop words.\"\"\"\n",
        "    words = []\n",
        "    for word in word_tokenize(text):\n",
        "        word = word.lower()\n",
        "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "        if word and word not in stop_words:\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            words.append(word)\n",
        "    return ' '.join(words)  # Join words back into a string\n",
        "\n",
        "# Preprocess the article text\n",
        "processed_article = preprocess_text(article1)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the processed article text\n",
        "vectorizer.fit([processed_article])\n",
        "\n",
        "# Transform the processed article text into a TF-IDF representation\n",
        "tfidf_representation = vectorizer.transform([processed_article])\n",
        "\n",
        "# Get the feature names (vocabulary)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the TF-IDF representation (sparse matrix)\n",
        "print(\"TF-IDF Representation:\")\n",
        "print(tfidf_representation.toarray())\n",
        "\n",
        "# Print the vocabulary\n",
        "print(\"\\nVocabulary:\")\n",
        "print(feature_names)\n",
        "\n",
        "# Example: Get the TF-IDF score of a specific word\n",
        "word_index = vectorizer.vocabulary_.get('mosquito')\n",
        "if word_index is not None:\n",
        "    tfidf_score = tfidf_representation[0, word_index]\n",
        "    print(f\"\\nTF-IDF score of 'mosquito': {tfidf_score}\")\n",
        "else:\n",
        "    print(\"\\n'mosquito' not found in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bi7z_fB68zce",
        "outputId": "50f4f6b2-0039-428e-f53a-15eec6fc2938"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation:\n",
            "[[0.04148699 0.04148699 0.04148699 0.04148699 0.04148699 0.08297398\n",
            "  0.08297398 0.04148699 0.04148699 0.08297398 0.04148699 0.04148699\n",
            "  0.04148699 0.08297398 0.04148699 0.04148699 0.08297398 0.04148699\n",
            "  0.08297398 0.04148699 0.08297398 0.04148699 0.29040893 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.08297398 0.08297398 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.24892194 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.08297398 0.08297398 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.04148699 0.24892194\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.04148699 0.04148699\n",
            "  0.08297398 0.04148699 0.04148699 0.04148699 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.04148699 0.08297398\n",
            "  0.16594796 0.04148699 0.04148699 0.08297398 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.08297398 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.16594796 0.08297398\n",
            "  0.04148699 0.04148699 0.12446097 0.04148699 0.04148699 0.4563569\n",
            "  0.04148699 0.20743495 0.04148699 0.04148699 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.20743495 0.04148699\n",
            "  0.08297398 0.04148699 0.16594796 0.04148699 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.08297398 0.08297398 0.04148699\n",
            "  0.04148699 0.04148699 0.08297398 0.04148699 0.04148699 0.04148699\n",
            "  0.12446097 0.04148699 0.12446097 0.04148699 0.04148699 0.08297398\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.08297398 0.08297398 0.04148699 0.04148699\n",
            "  0.08297398 0.04148699 0.04148699 0.04148699 0.04148699 0.04148699\n",
            "  0.04148699 0.04148699 0.04148699 0.04148699]]\n",
            "\n",
            "Vocabulary:\n",
            "['162hectare' '21' '44' '700' '70000' 'added' 'addition' 'alive' 'already'\n",
            " 'announced' 'another' 'applies' 'appreciates' 'area' 'asked' 'attempt'\n",
            " 'authority' 'aware' 'barangay' 'bashed' 'bbc' 'biggest' 'bounty'\n",
            " 'bringing' 'buildup' 'capital' 'carlito' 'case' 'cash' 'catching' 'cent'\n",
            " 'central' 'centre' 'cernal' 'chief' 'claimed' 'cleaning' 'coming'\n",
            " 'comment' 'community' 'concerned' 'consult' 'coordinate' 'crammed' 'dead'\n",
            " 'declined' 'defended' 'dengue' 'denguecarrying' 'dense'\n",
            " 'denselypopulated' 'department' 'died' 'disease' 'doh' 'drew' 'effective'\n",
            " 'egg' 'every' 'evidencebased' 'exchange' 'executive' 'existing'\n",
            " 'exterminated' 'far' 'farming' 'fight' 'five' 'follows' 'get' 'good'\n",
            " 'government' 'health' 'heart' 'help' 'hill' 'home' 'however' 'infection'\n",
            " 'intention' 'known' 'larva' 'late' 'lay' 'least' 'less' 'light' 'live'\n",
            " 'local' 'manila' 'meant' 'measure' 'medium' 'metro' 'month' 'mosquito'\n",
            " 'move' 'mr' 'nearly' 'necessary' 'neighbourhood' 'news' 'offer'\n",
            " 'offering' 'office' 'officer' 'one' 'patch' 'people' 'peso' 'philippine'\n",
            " 'please' 'pointed' 'practice' 'preventing' 'programme' 'provoked' 'read'\n",
            " 'recent' 'recorded' 'regional' 'rejected' 'reward' 'ridicule' 'rise'\n",
            " 'run' 'said' 'scorn' 'social' 'something' 'spike' 'spread' 'started'\n",
            " 'stop' 'stopping' 'street' 'student' 'supplement' 'surge' 'swift' 'told'\n",
            " 'total' 'town' 'tuesday' 'two' 'ultraviolet' 'urban' 'urge' 'using'\n",
            " 'village' 'water' 'way' 'wing' 'work']\n",
            "\n",
            "TF-IDF score of 'mosquito': 0.45635689750476227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings\n",
        "Using pre-trained embeddings (e.g., Word2Vec, GloVe) to represent words in the article as vectors."
      ],
      "metadata": {
        "id": "vQA6Bs1vDyJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "try:\n",
        "    wv = api.load('word2vec-google-news-300')\n",
        "except:\n",
        "    import ssl\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "    wv = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Preprocessing the text (same as before)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lowercase, remove punctuation, lemmatize, and remove stop words.\"\"\"\n",
        "    words = []\n",
        "    for word in word_tokenize(text):\n",
        "        word = word.lower()\n",
        "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "        if word and word not in stop_words:\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            words.append(word)\n",
        "    return words  # Return a list of words\n",
        "\n",
        "# Preprocess the article text\n",
        "processed_words = preprocess_text(article1)\n",
        "\n",
        "# Get word embeddings for each word in the article\n",
        "word_embeddings = []\n",
        "for word in processed_words:\n",
        "    try:\n",
        "        embedding = wv[word]  # Get the embedding for the word\n",
        "        word_embeddings.append(embedding)\n",
        "    except KeyError:\n",
        "        print(f\"Word '{word}' not found in the vocabulary.\")\n",
        "        # Handle words not found in the vocabulary (e.g., skip or use a default vector)\n",
        "\n",
        "# Now 'word_embeddings' is a list of vectors,\n",
        "# each representing a word in the processed article\n",
        "# You can use these vectors for further analysis or modeling.\n",
        "\n",
        "# Example: Print the embedding for the first word\n",
        "if word_embeddings:\n",
        "    print(\"Embedding for the first word:\")\n",
        "    print(word_embeddings[0])\n",
        "    print(\"Shape of embedding:\", word_embeddings[0].shape) # Print the shape of the embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99VwPHBo9F3I",
        "outputId": "137cd35c-9d54-4aaf-c62f-0ccb4cf9d8f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Word 'cernal' not found in the vocabulary.\n",
            "Word 'carlito' not found in the vocabulary.\n",
            "Word 'cernal' not found in the vocabulary.\n",
            "Word 'denselypopulated' not found in the vocabulary.\n",
            "Word 'centre' not found in the vocabulary.\n",
            "Word 'programme' not found in the vocabulary.\n",
            "Word 'cernal' not found in the vocabulary.\n",
            "Word 'neighbourhood' not found in the vocabulary.\n",
            "Word 'cernal' not found in the vocabulary.\n",
            "Word '21' not found in the vocabulary.\n",
            "Word '700' not found in the vocabulary.\n",
            "Word 'evidencebased' not found in the vocabulary.\n",
            "Word 'cernal' not found in the vocabulary.\n",
            "Word '44' not found in the vocabulary.\n",
            "Word '70000' not found in the vocabulary.\n",
            "Word '162hectare' not found in the vocabulary.\n",
            "Word 'cernal' not found in the vocabulary.\n",
            "Word 'denguecarrying' not found in the vocabulary.\n",
            "Embedding for the first word:\n",
            "[ 1.69677734e-02  2.34375000e-02  9.08203125e-02  2.83203125e-01\n",
            " -1.28906250e-01 -3.41796875e-03  1.53320312e-01 -5.59082031e-02\n",
            "  2.25830078e-02 -5.76171875e-02  1.96533203e-02 -1.34765625e-01\n",
            " -1.01562500e-01  1.01562500e-01 -9.76562500e-02  2.63671875e-02\n",
            "  1.61132812e-02  7.17773438e-02  9.47265625e-02  3.97949219e-02\n",
            " -5.41992188e-02 -1.77734375e-01  1.30859375e-01  4.68750000e-02\n",
            " -4.54101562e-02  9.27734375e-02 -3.58886719e-02  4.73632812e-02\n",
            " -1.94091797e-02 -4.51660156e-02  1.85546875e-02  5.83496094e-02\n",
            "  2.84423828e-02 -1.27410889e-03 -1.97265625e-01  1.32812500e-01\n",
            " -1.25976562e-01 -2.70996094e-02  8.25195312e-02 -1.69677734e-02\n",
            " -1.00585938e-01 -6.40869141e-03  5.20019531e-02  6.64062500e-02\n",
            "  8.34960938e-02  9.22851562e-02 -1.79687500e-01 -5.56640625e-02\n",
            "  5.12695312e-03 -2.25830078e-02 -1.83105469e-02  8.23974609e-03\n",
            "  1.68945312e-01  5.76171875e-02 -9.52148438e-02  9.57031250e-02\n",
            " -2.20703125e-01 -8.54492188e-02 -5.72204590e-05 -2.04101562e-01\n",
            "  3.44238281e-02 -1.32812500e-01 -9.76562500e-02  5.02929688e-02\n",
            "  1.30859375e-01 -2.25585938e-01 -5.59082031e-02 -1.87500000e-01\n",
            "  1.34277344e-02 -1.29394531e-02  1.83105469e-02  1.67968750e-01\n",
            " -3.14941406e-02  5.85937500e-02 -5.24902344e-02 -1.45874023e-02\n",
            "  7.81250000e-03  1.18164062e-01 -3.78417969e-03 -1.83105469e-02\n",
            " -1.03027344e-01  3.58886719e-02  4.71191406e-02  5.21850586e-03\n",
            " -3.17382812e-02  3.06396484e-02 -4.63867188e-02  1.30859375e-01\n",
            " -1.02050781e-01 -6.44531250e-02  3.32031250e-02  1.24023438e-01\n",
            " -1.32812500e-01 -1.04980469e-01 -1.27929688e-01  1.83105469e-02\n",
            "  6.25000000e-02  3.61328125e-02  2.69531250e-01 -2.99072266e-02\n",
            " -1.86523438e-01  2.42187500e-01  5.39550781e-02  1.66015625e-01\n",
            " -1.85546875e-01  1.02539062e-01 -9.71679688e-02 -1.28173828e-02\n",
            " -4.90722656e-02  5.59082031e-02  2.12402344e-02 -1.76757812e-01\n",
            " -1.25000000e-01 -3.73535156e-02  5.88989258e-03  2.56347656e-02\n",
            " -5.12695312e-02 -8.34960938e-02 -1.26342773e-02  3.90625000e-02\n",
            "  1.45507812e-01  6.39648438e-02 -7.17773438e-02  1.25732422e-02\n",
            "  1.21582031e-01  9.27734375e-02  1.79443359e-02  2.64892578e-02\n",
            "  1.53198242e-02  5.71289062e-02  5.34667969e-02  8.93554688e-02\n",
            "  1.10839844e-01  3.22265625e-02 -1.45507812e-01 -4.05883789e-03\n",
            " -1.40625000e-01 -9.42382812e-02  1.58203125e-01 -1.40625000e-01\n",
            "  1.63085938e-01 -2.33398438e-01  7.11059570e-03 -6.88476562e-02\n",
            "  7.76367188e-02 -1.64062500e-01 -7.17773438e-02  1.53198242e-02\n",
            "  8.44726562e-02 -6.03027344e-02  2.12890625e-01 -1.29882812e-01\n",
            " -1.12915039e-02 -1.68945312e-01 -6.78710938e-02 -2.81982422e-02\n",
            " -1.00097656e-01  1.65039062e-01  4.44335938e-02 -1.41601562e-02\n",
            "  1.06445312e-01  4.73022461e-03 -8.15429688e-02  3.70788574e-03\n",
            "  3.75976562e-02 -8.15429688e-02  8.69140625e-02  4.85839844e-02\n",
            " -1.87988281e-02  7.32421875e-02 -1.92382812e-01 -1.00585938e-01\n",
            "  8.15429688e-02 -1.28906250e-01 -7.03125000e-02  1.80664062e-02\n",
            "  8.34960938e-02 -1.04003906e-01  1.54296875e-01 -3.83300781e-02\n",
            "  3.02734375e-02 -3.08837891e-02 -8.05664062e-02  2.74658203e-02\n",
            " -6.98242188e-02  1.50146484e-02  5.54199219e-02  7.95898438e-02\n",
            "  3.34472656e-02  8.20312500e-02  6.07910156e-02  1.16699219e-01\n",
            " -8.59375000e-02  6.68945312e-02  1.34765625e-01  1.22070312e-01\n",
            " -3.49121094e-02  1.91650391e-02 -1.81884766e-02 -1.37695312e-01\n",
            " -1.53320312e-01  5.41992188e-02  7.32421875e-02 -1.98242188e-01\n",
            " -1.20849609e-02  1.72119141e-02 -1.23291016e-02 -2.33154297e-02\n",
            " -8.64257812e-02 -5.34667969e-02  8.78906250e-02  9.71679688e-02\n",
            " -8.42285156e-03 -2.86865234e-02 -1.61743164e-03  1.21459961e-02\n",
            "  5.49316406e-02  3.97949219e-02 -1.59912109e-02  6.29882812e-02\n",
            " -2.39257812e-02 -8.74023438e-02  9.58251953e-03  1.66015625e-02\n",
            "  1.18164062e-01  4.73632812e-02  5.51757812e-02  8.15429688e-02\n",
            " -1.72119141e-02  5.20019531e-02  5.66406250e-02 -9.17968750e-02\n",
            " -3.14941406e-02  1.01928711e-02 -3.32641602e-03  1.01318359e-02\n",
            "  1.32812500e-01 -9.76562500e-02  4.32128906e-02 -4.56542969e-02\n",
            "  7.95898438e-02  1.62506104e-03 -4.73632812e-02  1.00097656e-01\n",
            " -7.91015625e-02  2.69775391e-02 -3.58886719e-02  6.49414062e-02\n",
            " -1.06933594e-01 -1.84570312e-01  1.33789062e-01  5.41992188e-02\n",
            "  1.10839844e-01  6.54296875e-02  5.00488281e-02 -6.59179688e-02\n",
            "  5.00488281e-02  1.70898438e-02 -2.51770020e-03  3.45230103e-04\n",
            "  8.54492188e-03  9.15527344e-03  5.34667969e-02  1.05590820e-02\n",
            " -2.89306641e-02  2.35595703e-02 -1.14746094e-01  8.34960938e-02\n",
            " -4.58984375e-02 -1.19628906e-02 -1.80664062e-01 -3.18908691e-03\n",
            "  1.15234375e-01 -5.22460938e-02  1.08398438e-01  3.88183594e-02\n",
            " -1.12792969e-01 -1.54418945e-02 -6.54296875e-02  9.52148438e-02\n",
            " -1.36718750e-01  6.34765625e-02  3.10058594e-02 -2.34375000e-02\n",
            "  7.65991211e-03 -8.20312500e-02 -1.40625000e-01  1.81640625e-01\n",
            " -7.22656250e-02  9.21630859e-03 -5.63964844e-02  8.59375000e-02\n",
            " -4.56542969e-02  3.41796875e-02 -1.26953125e-01 -6.39648438e-02\n",
            " -1.13769531e-01  3.90625000e-02  1.02050781e-01  1.05957031e-01]\n",
            "Shape of embedding: (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "zUJVHndXEzP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Representation Methods: BoW, TF-IDF, and Word Embeddings**\n",
        "Comparing the representations obtained from BoW, TF-IDF, and Word Embeddings.\n",
        "\n",
        "#### **Bag of Words (BoW)**\n",
        "\n",
        "- **Representation**: Represents text as a collection of words and their frequencies. Each document is represented as a vector where each element corresponds to a word in the vocabulary, and the value is the frequency of that word in the document.\n",
        "  \n",
        "- **Pros**:\n",
        "  - Simple to understand and implement.\n",
        "  - Relatively efficient for large datasets.\n",
        "\n",
        "- **Cons**:\n",
        "  - Ignores word order and semantic relationships between words.\n",
        "  - Can lead to high dimensionality due to large vocabularies.\n",
        "\n",
        "- **Example**:\n",
        "  ```\n",
        "  Document: \"The quick brown fox jumps over the lazy dog.\"\n",
        "     Vocabulary: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]\n",
        "     BoW Representation: [2, 1, 1, 1, 1, 1, 1, 1]\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "\n",
        "- **Representation**: Similar to BoW but also considers the importance of words in the entire corpus. Words that are frequent in a specific document but rare in the overall corpus get higher weights.\n",
        "\n",
        "- **Pros**:\n",
        "  - Addresses some limitations of BoW by downweighting common words.\n",
        "  - Improves the relevance of words for document representation.\n",
        "\n",
        "- **Cons**:\n",
        "  - Still ignores word order and semantic relationships.\n",
        "  - Can be sensitive to document length.\n",
        "\n",
        "- **Example**:\n",
        "  ```\n",
        "  Document: \"The quick brown fox jumps over the lazy dog.\"\n",
        "     Vocabulary: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]\n",
        "     TF-IDF Representation: [0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
        "     (Illustrative values, actual values depend on corpus)\n",
        "  ```\n",
        "\n",
        "  Here, \"the\" has a lower weight because it's common in many documents.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Word Embeddings (e.g., Word2Vec, GloVe)**\n",
        "\n",
        "- **Representation**: Represents words as dense, low-dimensional vectors. Words with similar meanings are placed closer together in the vector space.\n",
        "\n",
        "- **Pros**:\n",
        "  - Captures semantic relationships between words.\n",
        "  - Can handle out-of-vocabulary words.\n",
        "  - Low dimensionality compared to BoW and TF-IDF.\n",
        "\n",
        "- **Cons**:\n",
        "  - More computationally expensive to train than BoW and TF-IDF.\n",
        "  - Requires large amounts of data for effective training.\n",
        "\n",
        "- **Example**:\n",
        "  ```\n",
        "  Word: \"king\"\n",
        "     Embedding: [0.25, 0.78, -0.12, ..., 0.55] (300-dimensional vector)\n",
        "  ```\n",
        "\n",
        "  Words like \"queen\", \"prince\", and \"royal\" would have embeddings close to \"king\" in the vector space.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Feature              | BoW                     | TF-IDF                  | Word Embeddings         |\n",
        "|----------------------|-------------------------|-------------------------|-------------------------|\n",
        "| **Representation**   | Word frequencies        | Word frequencies and importance | Dense vectors           |\n",
        "| **Word Order**       | Ignored                 | Ignored                 | Captured implicitly     |\n",
        "| **Semantics**        | Ignored                 | Ignored                 | Captured                |\n",
        "| **Dimensionality**   | High                    | High                    | Low                     |\n",
        "| **Computational Cost**| Low                     | Low                     | High (for training)     |\n"
      ],
      "metadata": {
        "id": "Q_z1VQG2_Phs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparing the representations obtained from BoW, TF-IDF, and Word Embeddings.**\n",
        "\n",
        "#### **Bag of Words (BoW)**\n",
        "\n",
        "- **Advantages**:\n",
        "  - **Simplicity**: BoW is easy to understand and implement. It represents text as a collection of words and their frequencies.\n",
        "  - **Efficiency**: BoW can be computationally efficient, especially for large datasets, as it involves simple counting of word occurrences.\n",
        "\n",
        "- **Limitations**:\n",
        "  - **Loss of Semantic Information**: BoW completely ignores the order of words and any relationships between them. For example, \"man bites dog\" and \"dog bites man\" would have the same BoW representation.\n",
        "  - **High Dimensionality**: The vocabulary size can be very large, leading to high-dimensional vectors. This can be computationally expensive and may lead to overfitting in machine learning models.\n",
        "  - **Inability to Handle Out-of-Vocabulary Words**: BoW cannot represent words that are not in its vocabulary. If a new word appears in the article, it won't be captured in the representation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "\n",
        "- **Advantages**:\n",
        "  - **Improved Relevance**: TF-IDF addresses some limitations of BoW by giving more weight to words that are important within a document but rare in the overall corpus. This helps identify words that are more relevant to the specific article.\n",
        "  - **Dimensionality Reduction**: Although still high-dimensional, TF-IDF can help reduce dimensionality compared to BoW, as it downweights common words.\n",
        "\n",
        "- **Limitations**:\n",
        "  - **Limited Semantic Understanding**: While TF-IDF considers word importance, it still ignores word order and semantic relationships. It cannot fully capture the meaning of phrases or sentences.\n",
        "  - **Sensitivity to Document Length**: TF-IDF can be sensitive to the length of the document. Longer documents may have higher TF-IDF scores for certain words, even if those words are not necessarily more important.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Word Embeddings (e.g., Word2Vec)**\n",
        "\n",
        "- **Advantages**:\n",
        "  - **Semantic Representation**: Word embeddings capture the semantic meaning of words by representing them as dense vectors in a continuous space. Words with similar meanings are placed closer together in this space.\n",
        "  - **Contextual Information**: Embeddings learn from the context in which words appear, allowing them to capture relationships between words and their meanings in different contexts.\n",
        "  - **Lower Dimensionality**: Compared to BoW and TF-IDF, word embeddings have much lower dimensionality, making them more efficient for computation.\n",
        "\n",
        "- **Limitations**:\n",
        "  - **Computational Cost**: Training word embeddings can be computationally expensive, especially for large datasets. Pre-trained embeddings like `word2vec-google-news-300` are often used to mitigate this issue.\n",
        "  - **Out-of-Vocabulary Words**: While embeddings can handle some out-of-vocabulary words by using similar words or context, they might not always capture the meaning of completely new or rare words.\n",
        "\n"
      ],
      "metadata": {
        "id": "QppEfh0uAIdy"
      }
    }
  ]
}